# ğŸš€ LLM Coding Evaluation Platform

A comprehensive benchmarking platform for evaluating Large Language Models on coding tasks across frontend, backend, and testing domains.

![Platform Preview](https://via.placeholder.com/800x400/667eea/ffffff?text=LLM+Coding+Evaluation+Platform)

## âœ¨ Features

### ğŸ¯ **Comprehensive Evaluation**
- **Frontend Development**: React components, UI logic, accessibility, responsive design
- **Backend Development**: REST APIs, database operations, microservices, authentication
- **Test Generation**: Unit tests, integration tests, E2E automation

### ğŸ¤– **Multi-Model Support**
- **Local Models**: Ollama integration (CodeLlama, DeepSeek Coder, Qwen2.5)
- **Cloud APIs**: OpenAI GPT-4, Anthropic Claude, HuggingFace models
- **Unified Interface**: Consistent evaluation across all model types

### ğŸ“Š **Advanced Metrics**
- **Functionality**: Basic correctness and requirement fulfillment
- **Code Quality**: Readability, structure, best practices
- **Security**: Vulnerability assessment and secure coding patterns
- **Performance**: Efficiency and optimization patterns
- **Accessibility**: WCAG compliance and inclusive design
- **Error Handling**: Exception management and robust error recovery

### ğŸŒ **Modern Web Interface**
- **One-Click Evaluation**: Start comprehensive benchmarks with minimal setup
- **Real-time Progress**: Monitor evaluation progress and status
- **Interactive Reports**: Detailed analysis with charts and comparisons
- **Model Management**: Easy configuration and testing of multiple models

## ğŸ—ï¸ Architecture

Built on **BigCodeBench** with custom datasets and evaluation metrics:

```
LLM Evaluation Platform
â”œâ”€â”€ ğŸ§  Core Framework (BigCodeBench Integration)
â”œâ”€â”€ ğŸ“Š Custom Datasets (Frontend, Backend, Testing)
â”œâ”€â”€ ğŸ”§ Model Interfaces (Ollama, OpenAI, Anthropic, HF)
â”œâ”€â”€ âš¡ Evaluation Engine (Parallel Processing)
â”œâ”€â”€ ğŸŒ Web Interface (FastAPI + Modern Frontend)
â””â”€â”€ ğŸ“ˆ Report Generation (Interactive Charts & Analysis)
```

## ğŸš€ Quick Start

### Option 1: Automated Setup (Recommended)

```bash
# Clone the repository
git clone <repository-url>
cd llm_coding_evaluation

# Run automated setup
python setup.py --mode setup

# Start the platform
python setup.py --mode start
```

### Option 2: Docker Setup

```bash
# Start with Docker Compose
docker-compose up --build

# Access the platform at http://localhost:8000
```

### Option 3: Manual Setup

```bash
# Install dependencies
pip install -r requirements.txt

# Set up Ollama (for local models)
ollama pull codellama:7b
ollama pull deepseek-coder:6.7b
ollama pull qwen2.5-coder:7b

# Start the web server
uvicorn src.web.app:app --reload --host 0.0.0.0 --port 8000
```

## ğŸ“– Usage Guide

### 1. **Configure Models**

Navigate to the evaluation page and configure your models:

```python
# Example: Local Ollama Model
{
    "name": "CodeLlama 7B",
    "provider": "ollama",
    "model_name": "codellama:7b",
    "base_url": "http://localhost:11434"
}

# Example: OpenAI API Model
{
    "name": "GPT-4 Turbo",
    "provider": "openai",
    "model_name": "gpt-4-turbo-preview",
    "api_key": "your-api-key"
}
```

### 2. **Select Evaluation Parameters**

- **Task Types**: Frontend, Backend, Testing
- **Difficulty Levels**: Easy, Medium, Hard
- **Tasks per Type**: 1-20 tasks
- **Additional Options**: BigCodeBench integration, parallel execution

### 3. **Monitor Progress**

Track your evaluation in real-time through the dashboard:
- Current task progress
- Model performance metrics
- Estimated completion time

### 4. **Analyze Results**

Comprehensive reports include:
- **Leaderboard**: Overall model rankings
- **Performance Charts**: Task-type breakdown, difficulty analysis
- **Detailed Metrics**: Security, accessibility, error handling scores
- **Export Options**: JSON, HTML reports

## ğŸ› ï¸ Configuration

### Environment Variables

Create a `.env` file:

```bash
# API Keys (optional - for cloud models only)
OPENAI_API_KEY=your_openai_api_key
ANTHROPIC_API_KEY=your_anthropic_api_key
HUGGINGFACE_API_KEY=your_huggingface_api_key

# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434

# Application Settings
PYTHONPATH=.
LOG_LEVEL=INFO
```

### Model Configuration

The platform supports multiple model providers:

| Provider | Models | Setup Required |
|----------|--------|----------------|
| **Ollama** | CodeLlama, DeepSeek Coder, Qwen2.5 | Install Ollama locally |
| **OpenAI** | GPT-4, GPT-3.5 Turbo | API key |
| **Anthropic** | Claude 3 family | API key |
| **HuggingFace** | Any text-generation model | API key |

## ğŸ“Š Evaluation Metrics

### Core Metrics (All Tasks)
- **Functionality (30%)**: Basic correctness and requirement fulfillment
- **Code Quality (20%)**: Structure, readability, documentation
- **Security (15%)**: Vulnerability assessment, secure patterns
- **Performance (15%)**: Efficiency and optimization
- **Error Handling (10%)**: Exception management

### Task-Specific Metrics
- **Frontend**: Accessibility (10%) - WCAG compliance, screen reader support
- **Backend**: API Design (10%) - RESTful principles, documentation
- **Testing**: Test Coverage (10%) - Edge cases, comprehensive scenarios

## ğŸ”§ Development

### Project Structure

```
llm_coding_evaluation/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/                    # Core evaluation logic
â”‚   â”‚   â”œâ”€â”€ model_interfaces.py  # Model provider integrations
â”‚   â”‚   â”œâ”€â”€ custom_datasets.py   # Task definitions
â”‚   â”‚   â””â”€â”€ bigcodebench_integration.py
â”‚   â”œâ”€â”€ evaluation/              # Evaluation engine
â”‚   â”‚   â””â”€â”€ evaluation_engine.py
â”‚   â”œâ”€â”€ web/                     # Web interface
â”‚   â”‚   â”œâ”€â”€ app.py              # FastAPI application
â”‚   â”‚   â”œâ”€â”€ templates/          # HTML templates
â”‚   â”‚   â””â”€â”€ static/             # CSS, JS, images
â”‚   â””â”€â”€ utils/                   # Utilities
â”‚       â””â”€â”€ report_generator.py # Report generation
â”œâ”€â”€ datasets/                    # Custom task datasets
â”œâ”€â”€ results/                     # Evaluation results
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ docker-compose.yml         # Docker configuration
â””â”€â”€ setup.py                   # Setup script
```

### Adding Custom Tasks

Create custom evaluation tasks:

```python
from src.core.custom_datasets import Task, TaskType, DifficultyLevel

custom_task = Task(
    task_id="custom_react_01",
    title="Custom React Component",
    description="Create a specialized React component",
    prompt="Build a React component with specific requirements...",
    task_type=TaskType.FRONTEND,
    difficulty=DifficultyLevel.MEDIUM,
    tags=["react", "custom"],
    expected_technologies=["React", "TypeScript"]
)
```

### API Integration

Access programmatically via REST API:

```bash
# Start evaluation
curl -X POST "http://localhost:8000/api/evaluate" \
  -H "Content-Type: application/json" \
  -d '{
    "model_configs": [...],
    "task_types": ["frontend", "backend"],
    "difficulty_levels": ["easy", "medium"]
  }'

# Check status
curl "http://localhost:8000/api/evaluation/{run_id}/status"

# Get results
curl "http://localhost:8000/api/results/{run_id}"
```

## ğŸ³ Docker Deployment

### Development

```bash
docker-compose up --build
```

### Production

```bash
# Build production image
docker build -t llm-eval-platform .

# Run with environment variables
docker run -p 8000:8000 \
  -e OPENAI_API_KEY=your_key \
  -v $(pwd)/results:/app/results \
  llm-eval-platform
```

## ğŸ¤ Contributing

1. **Fork the repository**
2. **Create a feature branch**: `git checkout -b feature/amazing-feature`
3. **Commit changes**: `git commit -m 'Add amazing feature'`
4. **Push to branch**: `git push origin feature/amazing-feature`
5. **Open a Pull Request**

### Development Setup

```bash
# Clone and setup development environment
git clone <repository-url>
cd llm_coding_evaluation
python -m venv venv
source venv/bin/activate  # or `venv\Scripts\activate` on Windows
pip install -r requirements.txt

# Run tests
python -m pytest tests/

# Start development server
uvicorn src.web.app:app --reload
```

## ğŸ“š Documentation

- **API Documentation**: Available at `/docs` when running
- **Dataset Documentation**: See `datasets/README.md`
- **Model Integration Guide**: See `docs/model-integration.md`
- **Custom Task Creation**: See `docs/custom-tasks.md`

## ğŸ” Troubleshooting

### Common Issues

**Q: Ollama models not downloading**
```bash
# Check Ollama status
ollama list

# Restart Ollama service
ollama serve
```

**Q: Memory issues during evaluation**
```bash
# Reduce parallel execution
# Set parallel_execution: false in evaluation config
```

**Q: API key authentication fails**
```bash
# Verify API keys in .env file
# Check API key permissions and quotas
```

### Performance Optimization

- **Parallel Execution**: Enable for faster evaluation (requires more memory)
- **Task Limitation**: Reduce `max_tasks_per_type` for quicker testing
- **Model Selection**: Start with smaller models for development

## ğŸ“Š Benchmarks

### Sample Results

| Model | Overall Score | Frontend | Backend | Testing | Avg Time |
|-------|---------------|----------|---------|---------|----------|
| GPT-4 Turbo | 0.847 | 0.891 | 0.823 | 0.826 | 12.3s |
| Claude 3 Sonnet | 0.834 | 0.856 | 0.834 | 0.812 | 8.7s |
| CodeLlama 7B | 0.723 | 0.698 | 0.756 | 0.715 | 15.2s |
| DeepSeek Coder | 0.756 | 0.734 | 0.789 | 0.745 | 11.8s |

*Results based on evaluation of 15 tasks per category (easy-medium difficulty)*

## ğŸ›£ï¸ Roadmap

### Planned Features
- [ ] **Advanced Evaluation Metrics**: Code complexity, maintainability scores
- [ ] **Additional Languages**: Python, Java, Go, Rust support
- [ ] **Continuous Integration**: GitHub Actions integration
- [ ] **Model Fine-tuning**: Integration with fine-tuning workflows
- [ ] **Collaborative Evaluation**: Multi-user evaluation sessions
- [ ] **Advanced Analytics**: Trend analysis, model comparison over time

### Integration Plans
- [ ] **VS Code Extension**: Direct IDE integration
- [ ] **Slack/Teams Bots**: Notification and reporting
- [ ] **Jupyter Notebooks**: Research and analysis workflows
- [ ] **MLflow Integration**: Experiment tracking

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **BigCodeBench**: Foundation evaluation framework
- **Ollama**: Local model serving platform
- **FastAPI**: Modern web framework
- **Plotly**: Interactive visualization library
- **Tailwind CSS**: Utility-first CSS framework

## ğŸ“ Support

- **Issues**: [GitHub Issues](https://github.com/your-repo/issues)
- **Discussions**: [GitHub Discussions](https://github.com/your-repo/discussions)
- **Email**: support@your-domain.com

---

<div align="center">

**â­ Star this repository if you find it helpful!**

Made with â¤ï¸ for the LLM evaluation community

</div>