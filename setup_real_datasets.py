#!/usr/bin/env python3
"""
Setup Real Public Datasets - PRODUCTION READY
Ensures all dependencies are installed for real dataset evaluation.
"""

import subprocess
import sys
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def install_package(package):
    """Install a Python package"""
    try:
        subprocess.run([sys.executable, "-m", "pip", "install", package], 
                      check=True, capture_output=True, text=True)
        logger.info(f"âœ… Installed {package}")
        return True
    except subprocess.CalledProcessError as e:
        logger.error(f"âŒ Failed to install {package}: {e}")
        return False


def check_and_install_dependencies():
    """Check and install required dependencies for public datasets"""
    
    logger.info("ğŸ” Checking dependencies for public datasets...")
    
    required_packages = [
        "datasets",  # For HumanEval, MBPP, CodeContests, APPS
        "transformers",  # Often required by datasets
        "torch",  # Often required by datasets
    ]
    
    installed = 0
    
    for package in required_packages:
        try:
            __import__(package)
            logger.info(f"âœ… {package} already installed")
            installed += 1
        except ImportError:
            logger.info(f"ğŸ“¦ Installing {package}...")
            if install_package(package):
                installed += 1
    
    logger.info(f"ğŸ“Š Dependencies: {installed}/{len(required_packages)} installed")
    return installed == len(required_packages)


def test_dataset_access():
    """Test access to public datasets"""
    
    logger.info("ğŸ§ª Testing dataset access...")
    
    try:
        from datasets import load_dataset
        
        # Test datasets
        test_datasets = [
            ("HumanEval", "openai_humaneval"),
            ("MBPP", "mbpp"),
            ("CodeContests", "deepmind/code_contests"),
            ("APPS", "codeparrot/apps")
        ]
        
        working = 0
        
        for name, dataset_id in test_datasets:
            try:
                logger.info(f"ğŸ” Testing {name}...")
                # Just check if we can access the dataset info
                dataset = load_dataset(dataset_id, split="test", streaming=True)
                next(iter(dataset))  # Try to get first item
                logger.info(f"âœ… {name}: Accessible")
                working += 1
            except Exception as e:
                logger.warning(f"âš ï¸ {name}: {str(e)[:100]}...")
        
        logger.info(f"ğŸ“Š Dataset Access: {working}/{len(test_datasets)} working")
        return working > 0  # At least one dataset should work
        
    except ImportError:
        logger.error("âŒ datasets library not available")
        return False


def main():
    """Setup public datasets for production use"""
    
    logger.info("ğŸš€ Setting up Real Public Datasets for LLM Evaluation")
    logger.info("=" * 60)
    
    # Step 1: Install dependencies
    logger.info("ğŸ“¦ Step 1: Installing dependencies...")
    if not check_and_install_dependencies():
        logger.error("âŒ Failed to install required dependencies")
        return False
    
    # Step 2: Test dataset access
    logger.info("\nğŸ§ª Step 2: Testing dataset access...")
    if not test_dataset_access():
        logger.error("âŒ No datasets accessible")
        logger.info("ğŸ’¡ This may be due to network issues or dataset availability")
        logger.info("ğŸ’¡ The platform will still work, but with limited datasets")
    
    # Step 3: Final status
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ‰ Setup Complete!")
    logger.info("ğŸ“‹ Available Datasets:")
    logger.info("   â€¢ HumanEval: 164 Python programming problems")
    logger.info("   â€¢ MBPP: 974 basic Python programming problems")
    logger.info("   â€¢ CodeContests: Programming contest problems") 
    logger.info("   â€¢ APPS: Python programming with test cases")
    logger.info("")
    logger.info("ğŸš€ Next Steps:")
    logger.info("   1. python app.py")
    logger.info("   2. Open: http://localhost:8000")
    logger.info("   3. Go to /evaluate page")
    logger.info("   4. Select models and start evaluation")
    logger.info("")
    logger.info("âœ¨ Features:")
    logger.info("   â€¢ Real datasets (no mock data)")
    logger.info("   â€¢ No authentication required")
    logger.info("   â€¢ Domain-specific evaluation")
    logger.info("   â€¢ Real-time progress tracking")
    
    return True


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
