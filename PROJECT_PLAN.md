# ğŸš€ LLM Coding Benchmark Platform

A comprehensive platform for evaluating open-source LLMs on Frontend, Backend, and Testing domains.

## ğŸ¯ Project Goals

Evaluate LLMs across 3 critical development domains:
- **Frontend**: React components, UI logic, styling, user interactions
- **Backend**: API endpoints, database operations, business logic, server functionality  
- **Testing**: Unit tests, integration tests, E2E testing scenarios

## ğŸ—ï¸ Architecture

Built on **BigCodeBench** foundation with domain-specific extensions:

```
Platform Architecture:
â”œâ”€â”€ BigCodeBench Core (Base evaluation engine)
â”œâ”€â”€ Domain-Specific Datasets
â”‚   â”œâ”€â”€ Frontend (React, Vue, Angular)
â”‚   â”œâ”€â”€ Backend (APIs, databases, auth)
â”‚   â””â”€â”€ Testing (Unit, integration, E2E)
â”œâ”€â”€ Multi-LLM Support (Ollama, OpenAI, Anthropic)
â”œâ”€â”€ Web UI (One-click evaluation)
â””â”€â”€ Results Dashboard (Leaderboards, analytics)
```

## ğŸ› ï¸ Implementation Plan

### Phase 1: Foundation Integration
1. âœ… Integrate BigCodeBench as evaluation engine
2. âœ… Configure multi-LLM support
3. âœ… Set up Docker environment

### Phase 2: Domain-Specific Datasets  
1. âœ… Frontend dataset (React components, UI interactions)
2. âœ… Backend dataset (REST APIs, database operations)
3. âœ… Testing dataset (Test generation, automation)

### Phase 3: Web Interface
1. âœ… One-click evaluation interface
2. âœ… Real-time progress tracking
3. âœ… Results dashboard with leaderboards

### Phase 4: Advanced Features
1. ğŸ”„ Additional benchmark integrations (HumanEval, EvalPlus)
2. ğŸ”„ Custom dataset upload
3. ğŸ”„ Comparative analysis tools

## ğŸš€ Quick Start

```bash
# Clone and setup
git clone <repository>
cd llm-coding-benchmark
docker-compose up --build

# Access web interface
open http://localhost:8000
```

## ğŸ“Š Supported Benchmarks

- **BigCodeBench**: Primary evaluation framework
- **HumanEval**: Function-level code generation
- **EvalPlus**: Extended test cases
- **Custom Datasets**: Domain-specific tasks

## ğŸ¤– Supported Models

- **Local**: Ollama (CodeLlama, DeepSeek, Qwen2.5-Coder)
- **API**: OpenAI, Anthropic, HuggingFace
- **Custom**: Any model via API interface
